# Развертывание кластера GlusterFS на Alt Linux

Данный проект предназначен для автоматизированного развертывания отказоустойчивого хранилища на базе **GlusterFS 11** в конфигурации 3 узла  **Replica 2 + 1 Arbiter**. Эта схема идеально подходит для небольшого проекта, обеспечивая целостность данных при минимальных затратах ресурсов на третьем узле.

---

## 1. Предварительная подготовка серверов

Перед запуском Ansible необходимо вручную подготовить три сервера (ноды) с установленной ОС **Alt Linux Server**.

### Требования к серверам:

1. **Пользователь**: Создан пользователь `administrator` (по умолчанию) с правами sudo.
2. **Сеть**: Серверы имеют статические IP-адреса и доступны друг другу по сети.
3. **Диски**: На каждом сервере установлен **отдельный пустой диск** (например, `/dev/sdb`) для нужд GlusterFS. **Внимание:** Плейбук сотрет данные на этом диске, если он не размечен!

---

## 2. Настройка проекта

1. **hosts.ini**: Отредактируйте файл, указав актуальные IP-адреса и имена дисков (`gluster_disk`). **ВНИМАНИЕ!** Перед первым запуском убедитесь, что переменная gluster_disk указывает на правильный диск! **Форматирование уничтожит** все данные на разделе.
2. **group_vars/gluster_nodes.yml**: Проверьте пути монтирования и имя тома.
3. **ansible.cfg**: Убедитесь, что `remote_user` совпадает с вашим именем пользователя.
4. **bootstrap.sh**: (Опционально) Запустите скрипт с необходимыми параметроми ( `bootstrap.sh -h` для справки) для настройки целевых серверов, либо проделайте данные настройки вручную (описаны ниже)



### Настройка SSH доступа:

На вашей управляющей машине (откуда запускаете Ansible) сгенерируйте ключ и скопируйте его на все узлы:

```bash
ssh-keygen -t ed25519
ssh-copy-id administrator@192.168.1.10
ssh-copy-id administrator@192.168.1.11
ssh-copy-id administrator@192.168.1.12

```

### Настройка Sudo (на каждом сервере):

Убедитесь, что пользователю разрешено выполнение команд от root. В Alt Linux добавьте пользователя в группу `wheel`:



```bash
su -
usermod -aG wheel administrator && echo '%wheel ALL=(ALL) ALL' | tee /etc/sudoers.d/wheel-group
chmod 0440 /etc/sudoers.d/wheel-group

```
или добавьте конкретному пользователю права sudo

```bash
su -
echo 'administrator ALL=(ALL:ALL) ALL' | tee /etc/sudoers.d/administrator
chmod 0440 /etc/sudoers.d/administrator

```

---


## 3. Запуск деплоя

Опционально: перед запуском деплоя запустите установку коллекции в папку проекта:

```bash
ansible-galaxy collection install -r requirements.yml -p ./collections

```


Для полного развертывания «с нуля» (подготовка дисков, установка GlusterFS, создание тома и монтирование на клиентах) выполните команду:


```bash
ansible-playbook site.yml -K

```
* -K запрашивает пароль sudo для пользователя administrator

### Что произойдет:

* Ansible проверит диски на наличие данных.
* Создаст разделы и отформатирует их в XFS.
* Установит серверную и клиентскую части GlusterFS.
* Объединит узлы в пул и создаст том `mail_vol`.
* Примонтирует диск в `/var/CommuniGate` на рабочих узлах.

---



### Вариант А: Запуск всего сценария (проверка идет в конце):

```bash
ansible-playbook site.yml -K

```

### Вариант Б: Запуск конкретных задач проверки:

пошаговый запуск

```bash
ansible-playbook site.yml --step

```

запуск начиная с определенной задачи 

```bash
ansible-playbook -i hosts.ini site.yml --start-at-task="Применение тюнинга производительности"
```

вывести список всех задач
```bash
ansible-playbook -i hosts.ini site.yml --list-tasks
```


## 4. Проверка состояния кластера

Если вы хотите запустить **только проверку** состояния (без внесения изменений), используйте встроенную роль проверки.

Вы можете запустить проверку отдельно, используя ограничение по тегам (если они добавлены) или напрямую через роль:

```bash
ansible -m include_role -a name=gluster_check cgp_clients -K

```

Или создайте файл `check.yml`:

```yaml
---
- hosts: cgp_clients
  become: yes
  roles:
    - gluster_check

```

И запускайте: `ansible-playbook check.yml -K`


---

## 5. Важные проверки после деплоя

После успешного завершения вы увидите отчет в консоли. Также вы можете выполнить ручные проверки на любом узле:

* **Состояние пула**: `sudo gluster peer status` (должно быть `Connected`).
* **Статус тома**: `sudo gluster volume status` (все Brick-порты должны быть активны).
* **Монтирование**: `df -h | grep CommuniGate` (должен отображаться сетевой диск).

---

## 6. Безопасность данных

Плейбук содержит **защитный механизм**. Если на диске `gluster_disk` будет обнаружена таблица разделов или данные, выполнение прервется с ошибкой. Это предотвращает случайную перезапись полезных данных.

Для переустановки на "грязный" диск необходимо предварительно очистить его вручную:

```bash
sudo wipefs -a /dev/sdb

```

---

### Дополнение: Инструкция по восстановлению

## 7. Восстановление узла при сбое

Если один из узлов вышел из строя (отказ оборудования/переустановка ОС), следуйте этой инструкции для возврата его в кластер.

### Шаг 7.1: Подготовка нового узла

1. Установите ОС Alt Linux.
2. Настройте сеть (IP должен быть тем же, что и у старого узла).
3. Создайте пользователя `administrator` и настройте `sudo` (см. раздел 1).
4. Убедитесь, что диск для GlusterFS пуст.

### Шаг 7.2: Запуск плейбука

Запустите основной деплой. Благодаря проверкам безопасности и идемпотентности, Ansible пропустит работающие узлы и настроит только новый сервер:

```bash
ansible-playbook site.yml -K --limit <имя_нового_узла>,<имя_исправного_узла>

```

*Важно: Мы включаем одну исправную ноду в `--limit`, чтобы новый узел смог выполнить `peer probe`.*

### Шаг 7.3: Восстановление идентификатора (UUID)

Если узел был заменен полностью, у него изменился внутренний UUID. Чтобы кластер принял его «кирпич» (brick), выполните на **исправном** узле:

1. Узнайте UUID нового узла: `gluster peer status`.
2. Если старый кирпич висит в статусе `Offline`, выполните переподключение:

```bash
# На исправном узле
gluster volume replace-brick mail_vol <старая_нода>:<путь> <новая_нода>:<путь> commit force

```

### Шаг 7.4: Запуск процесса самолечения (Self-heal)

GlusterFS должен синхронизировать данные с исправных узлов на новый.

```bash
# Запустить проверку всех файлов
gluster volume heal mail_vol full

# Проверить статус лечения (сколько файлов осталось перенести)
gluster volume heal mail_vol info

```

---

### Полезный совет для CommuniGate Pro

Поскольку CommuniGate активно работает с мелкими файлами писем, после восстановления узла рекомендуется убедиться, что индексные файлы (`.sub`, `.tmp`) синхронизированы полностью, прежде чем переключать на этот узел нагрузку.

### Оптимизация под CommuniGate Pro

оптимизация производительности прописана в задаче "Применение тюнинга производительности"

также примененные параметры можно посмотреть командой

```bash
gluster volume info mail_vol
```

### Как проверить, что всё восстановилось?

Выполните финальную роль из нашего проекта:

```bash
ansible-playbook site.yml -K --tags "check"

```

В отчете все "Bricks" должны быть в статусе **Online**, а в выводе `gluster peer status` все узлы должны быть **Connected**.

---

### Как это выглядит в структуре проекта:

```text
gluster_project/
├── ansible.cfg         # Глобальные настройки (sudo, пользователь)
├── hosts.ini           # Список серверов, IP и целевые диски
├── requirements.yml    # Список зависимостей коллекций
├── README.md           # Этот документ
├── site.yml            # Главный сценарий запуска
├── bootstrap.sh        # Скрипт предварительной подготовки целевых серверов
├── collections/         # Каталог коллекций
├── group_vars/         # Настройки путей и имен томов
└── roles/              # Модули логики
    ├── gluster_server/ # Подготовка дисков и создание кластера
    ├── gluster_client/ # Монтирование хранилища в /var/CommuniGate
    └── gluster_check/  # Сбор и вывод статуса системы
```

## License
This Ansible project is licensed under the MIT License. See the [LICENSE](../LICENSE) file for details.

Все роли и скрипты в этом репозитории распространяются под той же лицензией.